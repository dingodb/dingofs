##### mdsOpt
# RPC total retry time with MDS
mdsOpt.mdsMaxRetryMS=16000
# The maximum timeout of RPC communicating with MDS.
# The timeout of exponential backoff cannot exceed this value
mdsOpt.rpcRetryOpt.maxRPCTimeoutMS=2000
# RPC timeout for once communication with MDS
mdsOpt.rpcRetryOpt.rpcTimeoutMs=500
# RPC with mds needs to sleep for a period of time before each retry
mdsOpt.rpcRetryOpt.rpcRetryIntervalUS=50000
# Switch if the number of consecutive retries on the current MDS exceeds the limit.
# The number of failures includes the number of timeout retries
mdsOpt.rpcRetryOpt.maxFailedTimesBeforeChangeAddr=2
# The normal retry times for trigger wait strategy
mdsOpt.rpcRetryOpt.normalRetryTimesBeforeTriggerWait=3
# Sleep interval for wait
mdsOpt.rpcRetryOpt.waitSleepMs=1000
mdsOpt.rpcRetryOpt.addrs=127.0.0.1:6700,127.0.0.1:6701,127.0.0.1:6702  # __ANSIBLE_TEMPLATE__ {{ groups.mds | join_peer(hostvars, "mds_listen_port") }} __ANSIBLE_TEMPLATE__


#
# lease options
#
# client and mds lease time, default is 20s
mds.leaseTimesUs=20000000
mds.refreshTimesPerLease=5

#### metaCacheOpt
# Gets the number of retries for the leader
metaCacheOpt.metacacheGetLeaderRetry=3
# Need to sleep for a period of time before each get leader retry
metaCacheOpt.metacacheRPCRetryIntervalUS=100000
# RPC timeout of get leader
metaCacheOpt.metacacheGetLeaderRPCTimeOutMS=1000

#### executorOpt
# executorOpt rpc with metaserver
# rpc retry times with metaserver
executorOpt.maxRetry=10
# internal rpc retry times with metaserver
executorOpt.maxInternalRetry=3
# Retry sleep time between failed RPCs
executorOpt.retryIntervalUS=100000
# RPC timeout for communicating with metaserver
executorOpt.rpcTimeoutMS=1000
# RPC stream idle timeout
executorOpt.rpcStreamIdleTimeoutMS=500
# The maximum timeout RPC time of the retry request.
# The timeout time will follow the exponential backoff policy.
# Because the timeout occurs when the network is congested, the RPC timeout needs to be increased
executorOpt.maxRPCTimeoutMS=8000
# Maximum sleep time between retry requests.
# when the network is congested or the metaserver is overloaded,
# it is necessary to increase the sleep time
executorOpt.maxRetrySleepIntervalUS=8000000
executorOpt.minRetryTimesForceTimeoutBackoff=5
executorOpt.maxRetryTimesBeforeConsiderSuspend=20
# batch limit of get inode attr and xattr
executorOpt.batchInodeAttrLimit=10000

#### brpc
# close socket after defer.close.second
rpc.defer.close.second=1
# rpc health check interval in second, 0 or negative value means disable health check
rpc.healthCheckIntervalSec=0

#### fuseClient
# TODO(xuchaojie): add unit
fuseClient.listDentryLimit=65536
fuseClient.downloadMaxRetryTimes=3


# you shoudle enable it when mount one filesystem to multi mountpoints,
# it gurantee the consistent of file after rename, otherwise you should
# disable it for performance.
fuseClient.enableMultiMountPointRename=true
# thread number of listDentry when get summary xattr
fuseClient.listDentryThreads=10
# default data（s3ChunkInfo/volumeExtent） size in inode, if exceed will eliminate and try to get the merged one
fuseClient.maxDataSize=1024
# default refresh data interval 30s
fuseClient.refreshDataIntervalSec=30
fuseClient.warmupThreadsNum=10

# when read happen all block of file will be prefetch to block cache
# if no disk cache this option will be ignored
fuseClient.in_time_warmup=false

fuseClient.bthread_worker_num=0


fs.rpc.listDentryLimit=65536
fs.deferSync.delay=3
fs.deferSync.deferDirMtime=false
# }



#### block cache
# {
# block_cache.cache_store:
#   cache store type, none, disk or 3fs
#
# block_cache.stage_bandwidth_throttle_enable:
#   block will been put to s3 storage directly if disk write bandwidth
#   exceed limit.
#
# disk_cache.cache_dir:
#   directory for store cache block, multi directories
#   and corresponding max size are supported, e.g. "/data1:200;/data2:300"
#
# disk_cache.ioring_iodepth:
#   iodepth for io ring (works for both linux io uring and 3fs usrbio)
block_cache.cache_store=disk
block_cache.stage=true
block_cache.stage_bandwidth_throttle_enable=false
block_cache.stage_bandwidth_throttle_mb=10240
block_cache.logging=true
block_cache.upload_stage_workers=10
block_cache.upload_stage_queue_size=10000
block_cache.prefetch_workers=128
block_cache.prefetch_queue_size=10000

disk_cache.cache_dir=/var/run/dingofs  # __DINGOADM_TEMPLATE__ /dingofs/client/data/cache __DINGOADM_TEMPLATE__
disk_cache.cache_size_mb=102400
disk_cache.free_space_ratio=0.1
disk_cache.cache_expire_second=259200
disk_cache.cleanup_expire_interval_millsecond=1000
disk_cache.drop_page_cache=true
disk_cache.ioring_iodepth=128
disk_cache.ioring_blksize=1048576
disk_cache.ioring_prefetch=true

disk_state.tick_duration_second=60
disk_state.normal2unstable_io_error_num=3
disk_state.unstable2normal_io_succ_num=10
disk_state.unstable2down_second=1800
disk_state.disk_check_duration_millsecond=3000
# }

#### s3
# this is for test. if s3.fakeS3=true, all data will be discarded
s3.fakeS3=false
# prefetch blocks that disk cache use
s3.prefetchBlocks=1
# prefetch threads
s3.prefetchExecQueueNum=1
# start sleep when mem cache use ratio is greater than nearfullRatio,
# sleep time increase follow with mem cache use ratio, baseSleepUs is baseline.
s3.nearfullRatio=70
s3.baseSleepUs=500

# write cache < 8,388,608 (8MB) is not allowed
s3.writeCacheMaxByte=838860800
s3.readCacheMaxByte=209715200
# file cache read thread num
s3.readCacheThreads=5

s3.verify_SSL=False
s3.region=us-east-1
s3.maxConnections=32
s3.connectTimeout=60000
s3.requestTimeout=10000
# Off = 0,Fatal = 1,Error = 2,Warn = 3,Info = 4,Debug = 5,Trace = 6
s3.logLevel=4
s3.logPrefix=/data/logs/dingofs/aws_ # __DINGOADM_TEMPLATE__ /dingofs/client/logs/aws_ __DINGOADM_TEMPLATE__
# limit all inflight async requests' bytes, |0| means not limited
s3.maxAsyncRequestInflightBytes=104857600
# throttle
s3.throttle.iopsTotalLimit=0
s3.throttle.iopsReadLimit=0
s3.throttle.iopsWriteLimit=0
s3.throttle.bpsTotalMB=0
s3.throttle.bpsReadMB=0
s3.throttle.bpsWriteMB=0
s3.useVirtualAddressing=false
# The interval between read failures and retries will become larger and larger,
# and when the max is reached, retry will be performed at a fixed time.
s3.maxReadRetryIntervalMs = 1000
# retry interval
s3.readRetryIntervalMs = 100

s3.enableTelemetry=false

s3.use_crt_client=false
# this only work when use_crt_client is false
s3.use_thread_pool=true
# this only work when use_crt_client is false and use_thread_pool is true
s3.async_thread_num_in_thread_pool=256

#### common
client.common.logDir=/data/logs/dingofs  # __DINGOADM_TEMPLATE__ /dingofs/client/logs __DINGOADM_TEMPLATE__
# we have loglevel: {0,3,6,9}
# as the number increases, it becomes more and more detailed
client.loglevel=0
client.dummyServer.startPort=9000


mds.addr=172.20.61.102:7801

### uds
uds.fdCommPath=/var/run # unix domain socket file path